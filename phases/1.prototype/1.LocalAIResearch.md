# Phase Prototype

## 1. Local AI Research Story
### Description
An open source LLM AI application is needed to meet the requirements around keeping music scores private.  There is some concern around leaking data with the music scores themselves, not the research or prompts.  The music scores are copyrighted.

### Acceptance Criteria
- An AI platform is chosen that meets the business criteria outlined in [Requirements](../../REQUIREMENTS.md)
- An AI platform is up and running locally with web access.  **SkyNet has to start somewhere**.
- Prompts for research purposes can be handled by the AI
- LLMs must be available for the AI platform that are capable of understanding musical scores, even though using them is not part of this AC

### Notes
[OLLaMa](https://ollama.com/) is an widely popular LLM hosting service which is open source.  It is widely used for locally running LLMs to avoid public shared AIs for privacy purposes, and for avoiding expenses involved with API access per query to the popular AIs available by subscription on the internet.

The [AWS AI Services](https://aws.amazon.com/ai/services/) were also considered, but were rejected quite quickly due to the inability to host the services locally.  One of the unwritten purposes of this project is to learn more about AI and LLMs, and hosting them locally will allow for more learning, playing, and cost avoidance.

### Installation
Ollama [install](https://github.com/ollama/ollama) can be achieved via manual service installation, as docker containers, via windows linux subsytem package manager, or various other package managers such as pip.

After installation and launch, Ollama provides command line access to control the download and running of LLMs, very similar to docker.  Internet access is provided out of the box with Ollama and all the LLMs.  When the LLMs are running, prompting can by performed in various ways such as command line and API access.

The choice of LLM to run can be painful.  [Here]() is a chart of currently available and popular LLMs compatible with Ollama:

Here are some example models that can be downloaded:

|Model|Parameters|Size|Download|
|---------|-------|-------|-------------------|
|Gemma 3|1B|815MB|ollama run gemma3:1b|
|Gemma 3|4B|	3.3GB|	ollama run gemma3|
|Gemma 3|12B|	8.1GB|	ollama run gemma3:12b|
|Gemma 3|27B|	17GB|	ollama run gemma3:27b|
|QwQ|32B|	20GB|	ollama run qwq|
|DeepSeek-R1|7B|	4.7GB|	ollama run deepseek-r1|
|DeepSeek-R1|671B|	404GB|	ollama run deepseek-r1:671b|
|Llama 4|109B|	67GB|	ollama run llama4:scout|
|Llama 4|400B|	245GB|	ollama run llama4:maverick|
|Llama 3.3|70B|	43GB|	ollama run llama3.3|
|Llama 3.2|3B|	2.0GB|	ollama run llama3.2|
|Llama 3.2|1B|	1.3GB|	ollama run llama3.2:1b|
|Llama 3.2 Vision|11B|	7.9GB|	ollama run llama3.2-vision|
|Llama 3.2 Vision|90B|	55GB|	ollama run llama3.2-vision:90b|
|Llama 3.1|8B|	4.7GB|	ollama run llama3.1|
|Llama 3.1|405B|	231GB|	ollama run llama3.1:405b|
|Phi 4|14B|	9.1GB|	ollama run phi4|
|Phi 4 Mini|3.8B|	2.5GB|	ollama run phi4-mini|
|Mistral|7B|	4.1GB|	ollama run mistral|
|Moondream 2|1.4B|	829MB|	ollama run moondream|
|Neural Chat|7B|	4.1GB|	ollama run neural-chat|
|Starling|7B|	4.1GB|	ollama run starling-lm|
|Code Llama|7B|	3.8GB|	ollama run codellama|
|Llama 2 Uncensored|7B|	3.8GB|	ollama run llama2-uncensored|
|LLaVA|7B|	4.5GB|	ollama run llava|
|Granite-3.3|8B|	4.9GB|	ollama run granite3.3|

Note that as the number of parameters increases, the LLM size increases - making the LLM smarter, but that size also needs to fit in RAM.  So a normal local machine with 16Gigs or 32Gigs of RAM is limited to the really small LLMs.

### Music LLMs
Music capable LLMs compatible with Ollama were also found:
- [ChatMusician](https://arxiv.org/abs/2402.16153), a Cornell University project to read, write, and create music scores and music WAVs.  Also capable of understanding human speech, along with creating.
    - For music, it breaks down notes into ABC music notation, which is way of representing all notes as letters of the alphabet, and all other metadata around music scores represented with letters as well using key:value notation
    - [ChatMusician LLM](https://github.com/hf-lin/ChatMusician), the github location for the LLM implementation
- [MusicPrompts](https://ollama.com/ALIENTELLIGENCE/musicprompts), another example LLM that is music enabled
- [MusicLLM](https://ollama.com/tristanbehrens/musicllm), another example LLM that is music enabled
- [Music](https://ollama.com/Nazarii/music), another example LLM that is music enabled
- various others, even less well documented

### Interacting
Interacting with the LLMs on my desktop with a AMD 6750 XT GPU (3 year old video card) was returning from prompts in less than 10s, but showing around 3 words per second in the output.  This means that the research is performed from the internet in 10s, but it can take 2 minutes to show the results as it maps its thoughts back to english.  At this speed I felt like I was in a 1980s sci-fi film watching a teleprompter.  I dug thru the Ollama startup logs, and it was not using the GPU at all - 100% CPU only for the LLM.  This seems like a valid argument to go buy a new video card, but it would still be hard to sell this need to my wife.

On my gaming laptop with an nVidia 4070 GPU (1.5 year old video card), the same prompt started returning results in 10s, but was spewing results at around 15 words per second.  Not nearly as fast a cat'ing a file out to the screen, but faster than I can read it.

Example prompt:
- Plan an extended vacation to Alaska, including contact information for suggested night stay locations and prices
    - This is a difficult task due to its squishy nature and little control of where it will suggest
    - This is an interesting prompt since I just traveled with my family last summer to Alaska and witnessed my father perform this research.  I will not be the one to inform him of the 10s research response time of SkyNet.
    


